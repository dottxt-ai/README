---
title: README
subtitle: The Next Generation
author: The .txt team
author-url: "https://www.dottxt.co"
date: 2024-08-26
lang: en
version: v0.1.1
---

![](aspen.png)

There are hidden rules in every system.

Rules to govern the sentences we write, the software we code, and the music we play. These rules aren’t created to subjugate, but to integrate. Without them, the different parts of a system can’t work together to form something greater. 

Some believe that these rules reflect our unique way of thinking and conceiving of the world. But to computers alike, without structure there is chaos. Raw text, out of order, doesn’t make a program. You need a grammar — a syntax — to make the system work.

For almost a century, we’ve been teaching our machines the grammars of our world to help us work, play, and create better. But as of yet, our AIs remain stubbornly resistant to learning them. As smart as they appear to be, we haven’t found a way to make their outputs predictable (and thus useful) as part of a system. *It’s their fundamental flaw.*

We believe that this demands a new programming language, which allows LLMs to respond in a way that software understands. Until then, our generations are fun… but not  transformative. Until then, it’s just the beginning for how AI will change our world.

<hr>

Our mission is to help AI speak the language of every application

<hr>

.txt is a way to make LLMs reliable enough for the world to build on. A spellcheck for programming the future. And more broadly… an ecosystem for developers to design, execute, deploy, and evaluate LLM applications.

Most businesses aren’t run on natural language. If your goal is to sound conversational and believable enough, the everyday language works just fine. But to be good enough to compute with, the level of clarity must be far higher. In this sense, LLMs are, ironically, a regression!

LLMs are probabilistic by nature. We believe the only solution is implementing guard rails at the outset, based on your unique criteria… taking all potential probabilities and setting all the useless ones to zero. In other words, .txt stops LLM before they even make an error.

Fewer tokens. Less money. More efficiency.

We cut our teeth in applied statistical modeling — a job where you make bespoke models specially designed to give only correct answers. To us, applying this field to LLMs is the logical next step. The missing *layer* that will make AI truly useful.

The thing that makes LLMs act like… computers. Finally.

#

* Remi, Brandon and Dan
